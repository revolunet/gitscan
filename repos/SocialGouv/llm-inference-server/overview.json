{
  "url": "https://github.com/socialgouv/llm-inference-server",
  "name": "llm-inference-server",
  "description": "Ce projet fournit un serveur d'inférence pour les modèles de langage (LLM). Il permet de déployer et d'interroger des modèles hébergés sur Hugging Face ou S3. Il facilite l'utilisation de LLM pour diverses applications.",
  "language": "Python",
  "features": [
    "Inférence de modèles de langage",
    "Support de Hugging Face",
    "Support de S3",
    "API REST"
  ],
  "audience": "Professionnels",
  "dependencies": [
    "Poetry",
    "Transformers",
    "Torch",
    "Accelerate",
    "Boto3",
    "FastAPI",
    "Uvicorn",
    "Requests",
    "Tqdm",
    "Pytest"
  ],
  "components": [
    "Serveur d'inférence FastAPI",
    "Gestionnaire de modèles",
    "Clients S3 et Hugging Face",
    "Scripts de test"
  ],
  "auth": {
    "methods": [
      "none"
    ]
  },
  "tests": [
    "Pytest"
  ],
  "workflows": [
    "docker-build-push.yml"
  ],
  "lastActivity": "2025-03-06T04:01:41Z",
  "status": "active",
  "license": null,
  "hasDocumentation": true,
  "metrics": {
    "stars": 0,
    "forks": 1,
    "contributors": null,
    "openIssues": 2
  },
  "tags": [
    "LLM",
    "Inference",
    "Hugging Face",
    "S3",
    "FastAPI",
    "Python"
  ]
}
