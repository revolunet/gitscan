.
├── .claude
│   └── commands
│       ├── speckit.analyze.md
│       ├── speckit.checklist.md
│       ├── speckit.clarify.md
│       ├── speckit.constitution.md
│       ├── speckit.implement.md
│       ├── speckit.plan.md
│       ├── speckit.specify.md
│       └── speckit.tasks.md
├── .coderabbit.yaml
├── .cursor
│   └── commands
│       ├── speckit.analyze.md
│       ├── speckit.checklist.md
│       ├── speckit.clarify.md
│       ├── speckit.constitution.md
│       ├── speckit.implement.md
│       ├── speckit.plan.md
│       ├── speckit.specify.md
│       └── speckit.tasks.md
├── .dockerignore
├── .env.example
├── .github
│   └── workflows
│       ├── build_and_deploy.yml
│       └── pr_checks.yml
├── .gitignore
├── .pre-commit-config.yaml
├── .specify
│   ├── memory
│   │   └── constitution.md
│   ├── scripts
│   │   └── bash
│   │       ├── check-prerequisites.sh
│   │       ├── common.sh
│   │       ├── create-new-feature.sh
│   │       ├── setup-plan.sh
│   │       └── update-agent-context.sh
│   └── templates
│       ├── agent-file-template.md
│       ├── checklist-template.md
│       ├── plan-template.md
│       ├── spec-template.md
│       └── tasks-template.md
├── .windsurf
│   └── workflows
│       ├── speckit.analyze.md
│       ├── speckit.checklist.md
│       ├── speckit.clarify.md
│       ├── speckit.constitution.md
│       ├── speckit.implement.md
│       ├── speckit.plan.md
│       ├── speckit.specify.md
│       └── speckit.tasks.md
├── ACCEPTANCE_TESTING.md
├── CHANGELOG.md
├── CONTRIBUTING.md
├── Dockerfile
├── Dockerfile.dev
├── LICENSE
├── README.md
├── compose.dev.yml
├── compose.yml
├── docs
│   ├── .gitignore
│   ├── README.md
│   ├── docs
│   │   ├── developer-guide
│   │   │   ├── adding-a-new-metric.md
│   │   │   └── increase-parallel-requests.md
│   │   ├── getting-started
│   │   │   ├── install-from-source.md
│   │   │   └── install-with-docker.md
│   │   ├── index.md
│   │   └── user-guides
│   │       ├── add-your-dataset.md
│   │       ├── create-a-simple-experiment.md
│   │       ├── create-compliance-experiment.md
│   │       ├── create-experiment-set.md
│   │       ├── evaluate-your-own-ia-system.md
│   │       ├── run-answers-locally.md
│   │       ├── sample-your-dataset.md
│   │       └── use-custom-llm-judge-prompt.md
│   ├── docusaurus.config.js
│   ├── i18n
│   │   └── fr
│   │       ├── docusaurus-plugin-content-docs
│   │       │   └── current
│   │       │       ├── developer-guide
│   │       │       │   ├── adding-a-new-metric.md
│   │       │       │   └── increase-parallel-requests.md
│   │       │       ├── getting-started
│   │       │       │   ├── install-from-source.md
│   │       │       │   └── install-with-docker.md
│   │       │       ├── index.md
│   │       │       └── user-guides
│   │       │           ├── add-your-dataset.md
│   │       │           ├── create-a-simple-experiment.md
│   │       │           ├── create-experiment-set.md
│   │       │           ├── run-answers-locally.md
│   │       │           └── use-custom-llm-judge-prompt.md
│   │       └── docusaurus-plugin-content-pages
│   │           └── index.mdx
│   ├── package-lock.json
│   ├── package.json
│   ├── sidebars.js
│   ├── src
│   │   ├── components
│   │   │   └── Card
│   │   │       ├── CardBody
│   │   │       │   └── index.js
│   │   │       ├── CardFooter
│   │   │       │   └── index.js
│   │   │       ├── CardHeader
│   │   │       │   └── index.js
│   │   │       └── index.js
│   │   ├── css
│   │   │   └── custom.css
│   │   ├── pages
│   │   │   ├── api-docs.js
│   │   │   ├── index.mdx
│   │   │   └── redoc.js
│   │   └── theme
│   │       └── MDXComponents.js
│   └── static
│       └── img
│           ├── android-chrome-192x192.png
│           ├── android-chrome-512x512.png
│           ├── apple-touch-icon.png
│           ├── evalap-alpha-2.png
│           ├── evalap-alpha.png
│           ├── evalap-king.png
│           ├── evalap.png
│           ├── favicon-16x16.png
│           ├── favicon-32x32.png
│           ├── favicon.ico
│           └── leaderboard2-alpha.png
├── evalap
│   ├── __init__.py
│   ├── api
│   │   ├── __init__.py
│   │   ├── alembic
│   │   │   ├── README
│   │   │   ├── env.py
│   │   │   ├── script.py.mako
│   │   │   └── versions
│   │   │       ├── 0d299020f16a_add_dataset_columns.py
│   │   │       ├── 0f62332b614f_add_emission_carbon_in_observation.py
│   │   │       ├── 10a7f6d91de4_add_dataset_columns_map.py
│   │   │       ├── 114ab6896a16_mandatory_readmes.py
│   │   │       ├── 11c3d64a027a_compliance_dataset_information.py
│   │   │       ├── 1972b3f9769f_add_experiment_judge_model.py
│   │   │       ├── 2affae2e4758_locus_extra_df.py
│   │   │       ├── 2da85821482a_add_loadtesting_table.py
│   │   │       ├── 3473db3ff969_add_default_metric_in_dataset.py
│   │   │       ├── 4b903ee83b10_support_model_thinking.py
│   │   │       ├── 4c8e0b29de31_remove_obsolete_experiment_metrics_row.py
│   │   │       ├── 4de07d9a67df_add_locustrun.py
│   │   │       ├── 502a68292e65_num_line_unique_contraints.py
│   │   │       ├── 50b99f91d8d8_model_name_alias.py
│   │   │       ├── 5cc2750cfb90_merge_branches.py
│   │   │       ├── 60178b942bcb_answer_tool_call_steps.py
│   │   │       ├── 6f8e64af304c_merge_judge_and_emission_carbon_heads.py
│   │   │       ├── 72c0779e25da_add_with_vision_and_prelude_prompt.py
│   │   │       ├── 7c02444c083d_add_parquet_metadata_to_dataset_table.py
│   │   │       ├── 7e2ab5bde05b_moderaw_schema_for_users_given_output.py
│   │   │       ├── a8816ddd6dfd_add_created_at_for_answers.py
│   │   │       ├── a9542353ac06_add_dataset_sample.py
│   │   │       ├── cb8a2929e479_move_dataset_sample_to_experiment_sample.py
│   │   │       ├── d662c9622be1_add_optinal_context_in_answer.py
│   │   │       ├── da5b8dc15fcd_add_result_metric_params_field.py
│   │   │       ├── dcf62c3ab046_rename_prompt_system_to_system_prompt.py
│   │   │       ├── de6a4f1e935b_add_emission_carbon.py
│   │   │       ├── e2e82a623408_judge_model_relation.py
│   │   │       ├── edbdc05e2926_table_initialization.py
│   │   │       ├── eee3dc59f461_add_retrieval_context.py
│   │   │       ├── f4342e16e891_reomve_obsolete_dataset_fields.py
│   │   │       └── fc07604306e5_answer_nb_tool_calls_metric.py
│   │   ├── alembic.ini
│   │   ├── base.py
│   │   ├── config.py
│   │   ├── crud.py
│   │   ├── db.py
│   │   ├── endpoints.py
│   │   ├── errors.py
│   │   ├── main.py
│   │   ├── metrics
│   │   │   ├── __init__.py
│   │   │   ├── adhoc_judge.py
│   │   │   ├── generation_ops_metric.py
│   │   │   ├── judge_completude.py
│   │   │   ├── judge_complexity.py
│   │   │   ├── judge_exactness.py
│   │   │   ├── judge_notator.py
│   │   │   ├── judge_precision.py
│   │   │   ├── judge_rambling.py
│   │   │   ├── judge_relevant.py
│   │   │   ├── ocr_json.py
│   │   │   ├── output_length.py
│   │   │   └── qcm.py
│   │   ├── models.py
│   │   ├── schemas.py
│   │   └── security.py
│   ├── clients
│   │   ├── __init__.py
│   │   ├── llm.py
│   │   ├── mcp.py
│   │   └── schemas
│   │       ├── openai.py
│   │       ├── openai_rag.py
│   │       └── sampling_params.py
│   ├── config
│   │   ├── models-extra-info.toml
│   │   └── products
│   │       └── product_config.yml
│   ├── logger.py
│   ├── rag
│   │   ├── __init__.py
│   │   ├── build.py
│   │   ├── corpus_handler.py
│   │   ├── corpus_loader.py
│   │   ├── legalbench_chunk_v1.index.json
│   │   └── search.py
│   ├── runners
│   │   ├── __init__.py
│   │   ├── __main__.py
│   │   ├── dispatcher.py
│   │   ├── main.py
│   │   └── tasks.py
│   ├── scripts
│   │   ├── run_answers
│   │   │   ├── Readme.md
│   │   │   └── run_answers.py
│   │   ├── run_expe
│   │   │   └── run_expe.py
│   │   └── run_seed_data
│   │       └── __main__.py
│   ├── ui
│   │   └── demo_streamlit
│   │       ├── app.py
│   │       ├── experimentset_utils.py
│   │       ├── routes.py
│   │       ├── schemas.py
│   │       ├── static
│   │       │   └── images
│   │       │       ├── evalap_logo.png
│   │       │       └── evalap_logo_texte.png
│   │       ├── template_manager.py
│   │       ├── templates
│   │       │   ├── experiment_set.py.j2
│   │       │   ├── experiment_set.sh.j2
│   │       │   ├── experiment_set_cv.py.j2
│   │       │   └── experiment_set_cv.sh.j2
│   │       ├── ui_components.py
│   │       ├── utils.py
│   │       └── views
│   │           ├── __init__.py
│   │           ├── datasets.py
│   │           ├── experiments_set.py
│   │           ├── home.py
│   │           ├── launch_test_evaluation.py
│   │           ├── metrics.py
│   │           ├── ops.py
│   │           ├── test_standard_evaluation.py
│   │           └── test_your_own_evaluation.py
│   ├── utils.py
│   └── utils_eco.py
├── images
│   └── evalap_overview.png
├── justfile
├── notebooks
│   ├── RAG_evaluation.ipynb
│   ├── Readme.md
│   ├── create_legalbenchrag_dataset.ipynb
│   ├── create_marker_dataset.ipynb
│   ├── rd_change_rag_prompt.ipynb
│   ├── run_an_example_of_toolings_usage.ipynb
│   ├── run_dataset_experiences_demo.ipynb
│   ├── run_evals_compliance.ipynb
│   ├── run_evals_for_your_own_IA_system.ipynb
│   ├── run_evals_models_raw.ipynb
│   ├── run_evals_models_with_rag.ipynb
│   ├── run_evals_ocr_marker.ipynb
│   ├── run_evals_with_crossvalidation.ipynb
│   └── run_your_own_llm_as_a_judge_metric.ipynb
├── pyproject.toml
├── scripts
│   ├── drop_database.py
│   ├── drop_table.py
│   ├── get_experiment.py
│   ├── pray.sh
│   ├── publish_to_hf.py
│   ├── reset_experiment_status.py
│   ├── run_evalap.sh
│   ├── run_with_reload.py
│   └── start_uvicorn.sh
├── supervisord.conf
├── supervisord.dev.conf
├── tests
│   ├── __init__.py
│   ├── conftest.py
│   ├── endpoints
│   │   ├── test_datasets.py
│   │   ├── test_experiment_set.py
│   │   └── test_ops.py
│   ├── mock.py
│   ├── test_api.py
│   ├── test_utils_eco.py
│   └── utils
│       ├── __init__.py
│       ├── datasets.py
│       ├── expsets.py
│       └── ops.py
├── uv.lock
└── wiki
    ├── ocr.md
    └── tools.md
66 directories, 253 files
